<!DOCTYPE html>
<html>
  <head>
    <title>Artficial intellegence</title>
  </head>
  <body>
    <h2>Ethical machines</h2>
    <p>
      Machines with intelligence have the potential to use their intelligence to
      prevent harm and minimize the risks; they may have the ability to use
      ethical reasoning to better choose their actions in the world. As such,
      there is a need for policy making to devise policies for and regulate
      artificial intelligence and robotics.[214] Research in this area includes
      machine ethics, artificial moral agents, friendly AI and discussion
      towards building a human rights framework is also in talks.[215] Joseph
      Weizenbaum in Computer Power and Human Reason wrote that AI applications
      cannot, by definition, successfully simulate genuine human empathy and
      that the use of AI technology in fields such as customer service or
      psychotherapy[j] was deeply misguided. Weizenbaum was also bothered that
      AI researchers (and some philosophers) were willing to view the human mind
      as nothing more than a computer program (a position now known as
      computationalism). To Weizenbaum these points suggest that AI research
      devalues human life.[217]
    </p>
    <h2>Artificial moral agents</h2>
    <p>
      Wendell Wallach introduced the concept of artificial moral agents (AMA) in
      his book Moral Machines[218] For Wallach, AMAs have become a part of the
      research landscape of artificial intelligence as guided by its two central
      questions which he identifies as "Does Humanity Want Computers Making
      Moral Decisions"[219] and "Can (Ro)bots Really Be Moral".[220] For
      Wallach, the question is not centered on the issue of whether machines can
      demonstrate the equivalent of moral behavior, unlike the constraints which
      society may place on the development of AMAs.[221]
    </p>
    <h2>Machine ethics</h2>
    <p>
      AI is heavily used in robotics.[138] Advanced robotic arms and other
      industrial robots, widely used in modern factories, can learn from
      experience how to move efficiently despite the presence of friction and
      gear slippage.[139] A modern mobile robot, when given a small, static, and
      visible environment, can easily determine its location and map its
      environment; however, dynamic environments, such as (in endoscopy) the
      interior of a patient's breathing body, pose a greater challenge. Motion
      planning is the process of breaking down a movement task into "primitives"
      such as individual joint movements. Such movement often involves compliant
      motion, a process where movement requires maintaining physical contact
      with an object.[140][141][142] Moravec's paradox generalizes that
      low-level sensorimotor skills that humans take for granted are,
      counterintuitively, difficult to program into a robot; the paradox is
      named after Hans Moravec, who stated in 1988 that "it is comparatively
      easy to make computers exhibit adult level performance on intelligence
      tests or playing checkers, and difficult or impossible to give them the
      skills of a one-year-old when it comes to perception and
      mobility".[143][144] This is attributed to the fact that, unlike checkers,
      physical dexterity has been a direct target of natural selection for
      millions of years.[145]
    </p>
    <h2>Malevolent and friendly AI</h2>
    <p>
      Political scientist Charles T. Rubin believes that AI can be neither
      designed nor guaranteed to be benevolent.[224] He argues that "any
      sufficiently advanced benevolence may be indistinguishable from
      malevolence." Humans should not assume machines or robots would treat us
      favorably because there is no a priori reason to believe that they would
      be sympathetic to our system of morality, which has evolved along with our
      particular biology (which AIs would not share). Hyper-intelligent software
      may not necessarily decide to support the continued existence of humanity
      and would be extremely difficult to stop. This topic has also recently
      begun to be discussed in academic publications as a real source of risks
      to civilization, humans, and planet Earth. One proposal to deal with this
      is to ensure that the first generally intelligent AI is 'Friendly AI' and
      will be able to control subsequently developed AIs. Some question whether
      this kind of check could actually remain in place. Leading AI researcher
      Rodney Brooks writes, "I think it is a mistake to be worrying about us
      developing malevolent AI anytime in the next few hundred years. I think
      the worry stems from a fundamental error in not distinguishing the
      difference between the very real recent advances in a particular aspect of
      AI and the enormity and complexity of building sentient volitional
      intelligence."[225] Lethal autonomous weapons are of concern. Currently,
      50+ countries are researching battlefield robots, including the United
      States, China, Russia, and the United Kingdom. Many people concerned about
      risk from superintelligent AI also want to limit the use of artificial
      soldiers and drones.[226]
    </p>
    <ul>
      <li>
        <a href="index.html">Main page</a>
      </li>
      <li>
        <a href="applications.html">Applications</a>
      </li>
      <li>
        <a href="Language Processing.html">Language Processing</a>
      </li>
      <li>
        <a href="Social intellegence.html">Social intellegence</a>
      </li>
      <li>
        <a href="Ethical machines.html">Ethical machines</a>
      </li>
    </ul>
  </body>
</html>
